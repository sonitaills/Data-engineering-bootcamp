{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "opposite-television",
   "metadata": {},
   "source": [
    "## SQL SERVER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-acoustic",
   "metadata": {},
   "source": [
    "#COMO CORRIGIR O ERRO ABAIXO\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "#TÍTULO: Conectar ao Servidor  Não é possível conectar-se a DESKTOP-CF2LM59. \n",
    "#INFORMAÇÕES ADICIONAIS: Erro de rede ou específico à instância ao estabelecer conexão com o SQL Server. \n",
    "#O servidor não foi encontrado ou não estava acessível. Verifique se o nome da instância está correto e se o SQL Server \n",
    "#está configurado para permitir conexões remotas. (provider: Named Pipes Provider, error: 40 - Não foi possível abrir uma conexão com o SQL Server)\n",
    "#(Microsoft SQL Server, Erro: 2) Para obter ajuda, clique em: http://go.microsoft.com/fwlink?ProdName=Microsoft%20SQL%20Server&EvtSrc=MSSQLServer&EvtID=2&LinkId=20476 \n",
    "#O sistema não pode encontrar o arquivo especificado\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "#SOLUÇÃO\n",
    "#Youtube link --> https://www.youtube.com/watch?v=Jqb3untODYQ&t=4s"
   ]
  },
  {
   "cell_type": "raw",
   "id": "transparent-awareness",
   "metadata": {},
   "source": [
    "# mudando password do system administrator (sa)\n",
    "\n",
    "--> tps://community.boschsecurity.com/t5/Security-Intrusion/How-to-Reset-or-change-the-SA-Password-in-SQL-Server/ta-p/10193"
   ]
  },
  {
   "cell_type": "raw",
   "id": "muslim-brunei",
   "metadata": {},
   "source": [
    "# Autorizando usuário sa (enabling sa user)\n",
    "\n",
    "--> https://sudeeptaganguly.wordpress.com/2010/04/20/how-to-enable-sa-account-in-sql-server/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-potential",
   "metadata": {},
   "source": [
    "## DBEAVER CONNECTION TO SQL"
   ]
  },
  {
   "cell_type": "raw",
   "id": "electrical-rally",
   "metadata": {},
   "source": [
    "TCP/IP CONNECTION TO DATABASE ISSUE:\n",
    "\n",
    "--> https://pt.stackoverflow.com/questions/481737/conex%C3%A3o-do-sqlserver-localhost-no-dbeaver-erro-de-conex%C3%A3o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-cartoon",
   "metadata": {},
   "source": [
    "## LINUX"
   ]
  },
  {
   "cell_type": "raw",
   "id": "available-explanation",
   "metadata": {},
   "source": [
    "conda: command not found\n",
    "digite no terminal\n",
    "export PATH=~/anaconda3/bin:$PATH\n",
    "conda --version\n",
    "--> https://askubuntu.com/questions/908827/variable-path-issue-conda-command-not-found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-publicity",
   "metadata": {},
   "source": [
    "## FERRAMENTAS DE ETL"
   ]
  },
  {
   "cell_type": "raw",
   "id": "worth-latvia",
   "metadata": {},
   "source": [
    "ETL -> Elementos de composição\n",
    "--> Data lake é um repositário de dados essencialmente não estruturado\n",
    "--> Area staging = região onde ocorre transformação intermediária dos dados\n",
    "--> Data warehouse = base analítica. Aqui acontecem implementação de regras de negócios/ dimensões e fatos corporativos\n",
    "--> Data Marts = micro bases analíticas que implementam regras de negócios setoriais (pode ter um pra marketing, operações\n",
    "etc.\n",
    "-------Essa última camada é muita relacionada ao concetio de self-service BI-------"
   ]
  },
  {
   "cell_type": "raw",
   "id": "broke-norman",
   "metadata": {},
   "source": [
    "Um bom processo de ETL precisa de:\n",
    "--> Automatização (não deve depender de intervenção humana)\n",
    "--> Consistência de execução de jobs\n",
    "--> Encadeamento sequencial ou paralelizado de tarefas\n",
    "--> Possibilidade de conectar a diversas fontes (extração e entrega)\n",
    "--> Scheduler (programar execuções)\n",
    "--> Logs de execução (ajuda em bugs e no monitoramento de tempos de execução)\n",
    "--> Fail Safe (recuperação de dados em caso de falha)\n",
    "--> Notificação em caso de falha"
   ]
  },
  {
   "cell_type": "raw",
   "id": "labeled-purpose",
   "metadata": {},
   "source": [
    "Existem dois grandes tipos de ferramenta:\n",
    "Drag and drop --> Interface de usuário (Pentaho, Apache Nifi, Knimi, Talend)\n",
    ": Facilidade de uso --> quase nenhum códgio é utilizado\n",
    ": Podem ser amplamente usadas por parceiros nas áreas de negócios\n",
    ": \"Monte o seu pipeline\n",
    ": Podem ser utilizado por diversas áreas do negócio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-memory",
   "metadata": {
    "tags": []
   },
   "source": [
    "Soluções com código --> Desenvolvimento do pipeline através de framework (Kubeflow, Apache airflow, pachyderm, prefect, argo, apache, kafka, luigi)\n",
    ":Mais maleáveis e customizáveis\n",
    ": São implantadas e escalam com maior facilidade\n",
    ": São extensíveis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-peripheral",
   "metadata": {},
   "source": [
    "# Pentaho"
   ]
  },
  {
   "cell_type": "raw",
   "id": "interstate-remainder",
   "metadata": {},
   "source": [
    "Ferramenta amplamente utilizada em BI para integração de dados\n",
    "Interface de usuário muito amigável e conectores com diversas fontes de dados\n",
    "Indica durante a execução quais etapas já foram concluídas. Após a execução, mostra um diagrama de gantt com os tempos de execução de cada etapa \n",
    "Vantagens:\n",
    "--> Fácil de usar\n",
    "--> Vasta gama de conectores\n",
    "--> Dispõe de logs, métricas de execução e scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-drilling",
   "metadata": {},
   "source": [
    "## Apache NIFI"
   ]
  },
  {
   "cell_type": "raw",
   "id": "virtual-capitol",
   "metadata": {},
   "source": [
    "Ferramenta de automatização de pipeline de dados. Se propõe a atacar as seguintes dores:\n",
    "    --> Falhas de sistemas\n",
    "    --> Limitações insuficiente de dados (grandes ou pequenos demais, rápidos ou lentos demais)\n",
    "    --> Mudanças de comportamento dos dados\n",
    "    --> Complience e segurança\n",
    "    --> Melhoria contínua\n",
    "Este é um software que roda em cima de uma máquina virtual java.\n",
    "Acessado através do navegador!\n",
    "As caixinhas aqui são chamadas de \"processors\". Uma das vantangens é a variedade de processors para conectar com diversos tipos de arquivo e outra plataformas (?) (como por exemplo, Azure, Google Cloud, AWS)\n",
    "Vantagens:\n",
    "    --> Fácil de usar\n",
    "    --> Software gratuito (apache são todos gratuitos!)\n",
    "    --> Segurança da aplicação e consistência de execuções\n",
    "OBS: A instalação pode ser um pouco mais complicatado devido à virtual machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-april",
   "metadata": {},
   "source": [
    "## Apache Airflow"
   ]
  },
  {
   "cell_type": "raw",
   "id": "higher-lease",
   "metadata": {},
   "source": [
    "Iniciado em 2014 no Airbnb! Entrou no incubador da fundação Apache em 2016\n",
    "Plataforma para criar, schedular e monitorar workflows. Uma das ferramentas mais utilizadas hoje. Princípios norteadores:\n",
    "    --> Escalável: arquitetura modular que utiliza mensageria para orquestrar os workers (nós de computação que vão executar as tarefas)\n",
    "    --> Dinâmico: Pipelines definidas em Python, permitindo geração dinâmica\n",
    "    --> Extensível: fácil desenvolvimento de operadores e bibliotecas compatíveis (uma das principais vantagens)\n",
    "    --> Elegante: pipelines lean e explícitas\n",
    "    \n",
    "Features\n",
    "    --> Puro Python (pra tudo)\n",
    "    --> Interface do usuário simples e intuitiva\n",
    "    --> Integrações robustas (consegue comunicar com muita coisa)\n",
    "    --> Fácil de usar\n",
    "    --> Open source (hell yeah). Dá pra acessar o código pra verificar o funcionamento dele\n",
    "    \n",
    "SCHEDULER\n",
    "Crontab guru é um site que mostra o código para schedular atividades em determinado tempo.\n",
    "Ele funciona tipo o regex 101, mas só que para schedule!\n",
    "--> https://crontab.guru/every-2-minutes\n",
    "Ex: schedular para a cada 2 minutos (google it!)--> */2 * * * *\n",
    "schedule_interval =  */2 * * * *\n",
    "\n",
    "Comandos: \n",
    "--> iniciar e resetar airflow\n",
    "https://stackoverflow.com/questions/39073443/how-do-i-restart-airflow-webserver\n",
    "\n",
    "\n",
    "\n",
    "DÚVIDAS:\n",
    "Como configurar envio de e-mail automático no airflow no caso de falha?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-affiliate",
   "metadata": {},
   "source": [
    "## COMANDOS ÚTEIS --> AIRFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-passenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "Iniciar --> airflow webserver -p 8080 #pode escolher a porta que quiser. Deve ser iniciado da pasta onde se encontra o código"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-nation",
   "metadata": {},
   "source": [
    "## Comandos Úteis --> DOCKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-personal",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker ps --> Mostra os containers ativos\n",
    "docker-compose up -d  --> Cria dockers para executar airflow\n",
    "docker-compose up -d --scale worker=2 --> Cria 2 workers para executar tarefas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlling-maryland",
   "metadata": {},
   "source": [
    "## Kubeflow"
   ]
  },
  {
   "cell_type": "raw",
   "id": "curious-oxygen",
   "metadata": {},
   "source": [
    "Ferramenta amplamente associada à ciência de dados. Foi desenvolvida com o objetico de automatizar pipeline de machine learning escaláveis utilizando kubernetes.\n",
    "    --> Iniciado pelo google, vinculado à biblioteca Tensorflow\n",
    "    --> Oferece grande parte do pipeline de Ciência de dados\n",
    "    --> Encoraja soluções de ciências de dados utilizando arquitetura de microsserviços (desacoplada)\n",
    "    \n",
    "Componentes:\n",
    "    --> Dashboard de acompanhamento e gestão dos pipelines\n",
    "    --> Metadata: logs e monitoramento de execuções (estatísticas, etc)\n",
    "    --> Servidor de Jupter notebooks!\n",
    "    --> Fairing (biblioteca de automatização de treino e deploy de ML)\n",
    "    --> Feature store (feast): Armazaaenmento de features já processadas\n",
    "    --> Frameworks disponíveis para desenvolvimento de machine learning (PyTorch, tensorflow, etc)\n",
    "    --> Tuning de hiperparâmetros (parâmetros que dão melhor desempenho do seu ML)\n",
    "    --> KF pipelines: ferramenta de orquestração de tarefas\n",
    "    --> KF serving: Ferramenta para servir modelos de ML como API só que em cima de uma estrutura de kubernetes.\n",
    "    \n",
    "Vantagens\n",
    "    --> Tecnologia Kubernetes (escalável, alta disponibilidade, automatização_\n",
    "    --> Agrega todas as partes de uma equipe de data analytics\n",
    "    --> Pensado para ambientes de produção\n",
    "    --> Equipes com maior experiência em Kubernetes tendem a ter melhores resultados com essa ferramenta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-timing",
   "metadata": {},
   "source": [
    "## Prefect"
   ]
  },
  {
   "cell_type": "raw",
   "id": "complex-hawaiian",
   "metadata": {},
   "source": [
    "Ferramenta sendo muito utilizada no mundo.\n",
    "    --> Proposta de ser o jeito mais fácil para automatizar dataflow\n",
    "    --> Lógica pareceida com o airflow\n",
    "    --> O grande diferencial é o serviço de cloud próprio, com vários níveis de assinatura (inclusive gratuito) para servir de backend para as execuções\n",
    "    \n",
    "Arquitetura:\n",
    "    --> UI: interface para iteração\n",
    "    --> Apollo: Endpoint para interação com o servidor\n",
    "    --> PostgreSQL: camada para persistência de dados\n",
    "    --> Hasura: GraphQL API para consultas aos metadados\n",
    "    --> GraphQL: regras de negócio do servidor.\n",
    "    --> Towel: Utilitários:\n",
    "        --> Scheduler\n",
    "        --> Zombie killer: marca tarefas como falha\n",
    "        --> Lazarus: Reagendar execuções com estado não usual por um certo período"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-native",
   "metadata": {},
   "source": [
    "## Atronomer.io --> Certificação grátis do airflow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metallic-matter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
